\documentclass{article}
\usepackage{minted}
\usepackage{amsmath}
\begin{document}

\author{Wang Cheng}
\title{COMP 9102 Assignment 3}
\date{}
\maketitle

\section{Implementation}
\subsection{Building the matrix}
Since we treat retweet data as an undirected graph, the matrix should be 
symmetric.

\begin{minted}
[
frame=lines
]
{python}
for row in data:
  matrix[row[0] - 1, row[1] - 1] = 1
  matrix[row[1] - 1, row[0] - 1] = 1
\end{minted}

\subsection{Personal page-rank}
PPR-based proximity vector for node u is defined as follows: $p_{u} = (1 - 
\alpha)Ap_{u} + \alpha e_{u}$. By setting $p^{(0)}_{u} = \mathbf{0}$, $p_u$ is 
computed iteratively.

\subsection{Evaluation of clustering}

We evaluated the quality of clustering using the label file by 
following criteria:

\textbf{entropy} For each cluster, the class distribution of the data is 
calculated first, i.e., for cluster $j$ we compute $p_{ij}$, the 
'probability' that a member of cluster $j$ belongs to class $i$ as follows: 
$p_{ij} = m_{ij} / m_{j}$, where $m_{j}$ is the number of values in cluster $j$ 
and $m_{ij}$ is the number of values of class $i$ in cluster $j$. Then using 
this class distribution, the \textbf{entropy of each cluster} $j$ is calculated 
using the standard formula $e_{j} = \sum_{i=1}^{L} p_{ij} \log_2 p_{ij}$, where 
$L$ is the number of classes. The \textbf{total entropy for a set of clusters} 
is calculated as the sum of the entropies of each cluster weighted by the size 
of each cluster, i.e., $e = \sum_{i=1}^{K} \frac{m_i}{m} e_j$, where $m_{j}$ is 
the size of cluster $j$, $K$ is the number of clusters, and $m$ is the total 
number of data points.

\textbf{purity} Using the terminology derived for entropy, the purity of cluster 
j, is given by $purity_{j} = \text{max} \, p_{ij}$ and the overall purity of a 
clustering by $purity = \sum_{i=1}^{K} \frac{m_i}{m} $

\textbf{NMI} Normalized Mutual Information $NMI(Y,C) = \frac{I(Y;C)}{[H(Y) +  
H(C)] / 2} $ is calculated by the following steps (All logs are base-2):
\begin{enumerate}
  \item Calculate entropy of class labels
  \item Calculate entropy of cluster labels
  \item Calculate mutual information by $I(Y,C) = H(Y) - H(Y|C) $, where 
H(Y|C)is the conditional entropy of class labels for clustering
  \item Calculate NMI
\end{enumerate}

\section{Experiment}

To run the code: \texttt{python assignment3.py -k 5}. Here, \texttt{-k} is the 
number of clusters to form.

\begin{table}[tbh]
  \center
  \footnotesize
  \begin{tabular}{l|c|c|c}
     & Purity & Entropy & Normalized mutual information (NMI) \\
    \hline
    k = 5 & 11 & 11 & 11 \\
    \hline
    k = 10 & 11 & 11 & 11 \\
    \hline
    k = 15 & 11 & 11 & 11 \\
  \end{tabular}
\end{table}

\end{document}
