from __future__ import division
import argparse
import numpy as np
from sklearn.cluster import KMeans

if __name__ == "__main__":

	parser = argparse.ArgumentParser(description = "Using Clustering for Community Search")

	parser.add_argument('-k', type = int, help = "number of clusters", required = True)

	args = parser.parse_args()

	graph = np.loadtxt('Graph.txt', dtype = int)

	N = np.amax(graph)
	
	matrix = np.zeros(shape = (N, N))

	for row in graph:
		# We treat retweet data as an undirected graph.
		matrix[row[0] - 1, row[1] - 1] = 1
		matrix[row[1] - 1, row[0] - 1] = 1

	A = np.zeros(shape = (N, N))
	for i in xrange(0, N):
		for j in xrange(0, N):
			if matrix[i, j] == 1:
				A[i, j] = 1 / np.sum(matrix[j])

	alpha = 0.1
	convergence = 0.00001

	for i, row in enumerate(matrix):
		e_u = np.zeros(N)
		for j in xrange(0, N):
			if row[j] == 1:
				e_u[j] = 1 / np.sum(row)

		err = float('inf')
		p_u = np.zeros(N)
		while err > convergence:
			tmp = p_u
			p_u = (1 - alpha) * np.dot(A, p_u) + alpha * e_u
			err = np.sum(abs(p_u - tmp))

		p = np.vstack((p, p_u)) if i != 0 else p_u

	kmeans = KMeans(n_clusters = args.k).fit(p)

	eval_matrix = np.hstack((kmeans.labels_[None].T, np.full((N, 1), np.inf)))

	labels = np.loadtxt('Labels.txt', dtype = int)

	for row in labels:
		eval_matrix[row[0] - 1, 1] = row[1]

	# entropy
	for cluster in xrange(0, args.k):
		u, counts = np.unique(eval_matrix[eval_matrix[:, 0] == cluster][:, 1], return_counts = True)
		m_j[cluster] = counts[0] + counts[1]
		e_j[cluster] = counts[0] / m_j[cluster] * np.log2(counts[0] / m_j[cluster]) + counts[1] / m_j[cluster] * np.log2(counts[1] / m_j[cluster])

		purity_j = max(counts[0] / m_j[cluster], counts[1] / m_j[cluster])

	entropy = 0
	for cluster in xrange(0, args.k):
		entropy += (m_j[cluster] / sum(m_j.values())) * e_j[cluster]

	print "entropy is", entropy

	# purity
	purity = 0
	for cluster in xrange(0, args.k):
		purity += (m_j[cluster] / sum(m_j.values())) * purity_j

	print "purity is", purity

	# NMI
	entroy_ClassLabels = np.count_nonzero(labels[:, 1] == 0) / sum(m_j.values()) * np.log2(np.count_nonzero(labels[:, 1] == 0) / sum(m_j.values())) + np.count_nonzero(labels[:, 1] == 1) / sum(m_j.values()) * np.log2(np.count_nonzero(labels[:, 1] == 1) / sum(m_j.values()))
	entroy_ClassLabels = -entroy_ClassLabels

	entroy_ClusterLabels = 0
	for cluster in xrange(0, args.k):
		entroy_ClusterLabels += m_j[cluster] / sum(m_j.values()) * np.log2(m_j[cluster] / sum(m_j.values()))
	
	entroy_ClusterLabels = -entroy_ClusterLabels

	for cluster in xrange(0, args.k):
		conditional_entropy[cluster] = -(m_j[cluster] / sum(m_j.values())) * e_j[cluster]

	mutual_information = entroy_ClassLabels - sum(conditional_entropy.values())

	NMI = mutual_information / ((entroy_ClassLabels + entroy_ClusterLabels) / 2)

	print "NMI is", NMI
